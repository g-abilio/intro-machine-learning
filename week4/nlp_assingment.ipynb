{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34131827-a163-4bdd-862d-984a5ff07cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchtext==0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de86b84-be34-4a40-ab50-c47163d70a4e",
   "metadata": {},
   "source": [
    "Uploading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1a9043-782c-4ea9-a7a5-62a3518edf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:01, 63537.39lines/s]\n",
      "120000lines [00:03, 35966.08lines/s]\n",
      "7600lines [00:00, 38329.06lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "ngrams = 1\n",
    "train_csv_path = 'ag_news_csv/train.csv'\n",
    "test_csv_path = 'ag_news_csv/test.csv'\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    torchtext.datasets.text_classification._csv_iterator(train_csv_path, ngrams))\n",
    "train_data, train_labels = torchtext.datasets.text_classification._create_data_from_iterator(\n",
    "        vocab, torchtext.datasets.text_classification._csv_iterator(train_csv_path, ngrams, yield_cls=True), False)\n",
    "test_data, test_labels = torchtext.datasets.text_classification._create_data_from_iterator(\n",
    "        vocab, torchtext.datasets.text_classification._csv_iterator(test_csv_path, ngrams, yield_cls=True), False)\n",
    "if len(train_labels ^ test_labels) > 0:\n",
    "    raise ValueError(\"Training and test labels don't match\")\n",
    "agnews_train = torchtext.datasets.TextClassificationDataset(vocab, train_data, train_labels)\n",
    "agnews_test = torchtext.datasets.TextClassificationDataset(vocab, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f06ec01-5170-4000-8479-5570dfb9dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, tensor([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,    14,\n",
      "           28,    15,    28,    16, 50726,     4,   432,   375,    17,    10,\n",
      "        67508,     7, 52259,     4,    43,  4010,   784,   326,     2]))\n"
     ]
    }
   ],
   "source": [
    "print(agnews_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b753f8a4-ba1f-4064-a8ef-c7dfe75d670b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agnews_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049f18b9-6d2b-4814-8d47-087c679b5953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agnews_train[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec8fe9-ee78-4766-8601-3c5207f889cf",
   "metadata": {},
   "source": [
    "Padding and truncating to make them the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9465262b-ccf1-4085-9bc5-9457823958cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sequence padded: tensor([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,    14,\n",
      "           28,    15,    28,    16, 50726,     4,   432,   375,    17,    10,\n",
      "        67508,     7, 52259,     4,    43,  4010,   784,   326,     2,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "First sequence length: 42\n",
      "Second sequence padded: tensor([15875,  1073,   855,  1311,  4251,    14,    28,    15,    28,    16,\n",
      "          930,   798,   321, 15875,    99,     4, 27658,    29,     6,  4460,\n",
      "           12,   565, 52791,     9, 80618,  2126,     8,     3,   526,   242,\n",
      "            4,    29,  3891, 82815,  6575,    11,   207,   360,     7,     3,\n",
      "          127,     2])\n",
      "Second sequence length: 42\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "padded_exs = pad_sequence([agnews_train[0][1], agnews_train[1][1]])\n",
    "print(\"First sequence padded: {}\".format(padded_exs[:,0]))\n",
    "print(\"First sequence length: {}\".format(len(padded_exs[:,0])))\n",
    "print(\"Second sequence padded: {}\".format(padded_exs[:,1]))\n",
    "print(\"Second sequence length: {}\".format(len(padded_exs[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058eff31-c5d9-4302-b784-51e176b36a1e",
   "metadata": {},
   "source": [
    "Creating the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddfc7da4-3750-4a5a-a419-149288a01611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def collator(batch):\n",
    "    labels = torch.tensor([example[0] for example in batch])\n",
    "    sentences = [example[1] for example in batch]\n",
    "    data = pad_sequence(sentences)\n",
    "    \n",
    "    return [data, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592ed431-e85c-4106-958a-68ec60919904",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(agnews_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "test_loader = torch.utils.data.DataLoader(agnews_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236edb6-f976-4d89-bb50-5571793bc5ac",
   "metadata": {},
   "source": [
    "Hyperparameters defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34486c6c-b927-4902-a4ef-91947673e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(agnews_train.get_vocab())\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_OUTPUTS = len(agnews_train.get_labels())\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771837dc-a3f6-4242-9e2f-a44bda0758c1",
   "metadata": {},
   "source": [
    "SWEM example. Here, the word vector parameters are learned as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d7b4ea-9464-406e-9276-449c7c301981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SWEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        embed_mean = torch.mean(embed, dim=0)\n",
    "        \n",
    "        h = self.fc1(embed_mean)\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18711bf5-21ee-4ae4-ae43-23299b8d2ce3",
   "metadata": {},
   "source": [
    "Training and evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55eb5435-500d-4f15-918a-7043b1524a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SWEM(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ec9642e-7bea-48f3-b587-c5e4b248a15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638bca5b8e81418385287ec29f3a60c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aacaa8e5a243c9bbd345d6a89a8369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa55f44d48a046d09632314e96be90b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5514c2eb1aa9464d971cda87235823c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9019736647605896\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Cross entropy (CE) Loss and Adam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.argmax(y,dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())        \n",
    "        num_examples += len(labels)        \n",
    "    # Print training progress\n",
    "    # Not working for some reason\n",
    "    ##if epoch % 25 == 0:\n",
    "      ##  acc = correct/num_examples\n",
    "        ##print(\"Epochs {0}: \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        # Forward pass\n",
    "        y = model(inputs)        \n",
    "        predictions =  torch.argmax(y,dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(labels) \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11fa71f6-aa9a-484c-a098-46b782edf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9587924\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params_SWEM = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params_SWEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa929f-fbbb-4d54-bdda-c184a61e982b",
   "metadata": {},
   "source": [
    "Let's try a RNN now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ab06f0-776c-4867-a41f-e392eff148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        output, h_final = self.rnn(embed)\n",
    "        output = self.fc2(h_final.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e0c03-2f0c-4c81-a028-8447ab4dfeb0",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0865225e-bd2e-47ce-9c6b-76c811b06174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RNN(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8445f27-875c-41f0-8fd8-72a7b8381907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy (CE) Loss and Adam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model_rnn(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.argmax(y,dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())        \n",
    "        num_examples += len(labels)        \n",
    "    # Print training progress\n",
    "    # Not working for some reason\n",
    "    ##if epoch % 25 == 0:\n",
    "      ##  acc = correct/num_examples\n",
    "        ##print(\"Epochs {0}: \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        # Forward pass\n",
    "        y = model_rnn(inputs)        \n",
    "        predictions =  torch.argmax(y,dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(labels) \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56bd183-3b4f-40e7-af28-c7635e31cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592084\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model_rnn.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
