{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a415ac-eb47-4c09-a75e-750403f3fe62",
   "metadata": {},
   "source": [
    "Uploading pre-trained word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8460064b-9d5d-4bdd-aa9b-1638444f7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Conceptnet Numberbatch word embeddings...\n",
      "let's go\n",
      "working...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('datasets/mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\") \n",
    "    conceptnet_url = \"http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5\"\n",
    "    urlretrieve(conceptnet_url, \"mini.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53dee998-3cf9-418c-aa38-4967db1b92dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/gabilio/anaconda3/envs/pytorch/lib/python3.11/site-packages (from h5py) (1.26.4)\n",
      "Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5py\n",
      "Successfully installed h5py-3.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ee3c2-92f6-48ca-bcf3-bba66267423f",
   "metadata": {},
   "source": [
    "Opening the mini.h5 file and extracting uft-8 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9970e2b-a3b6-41ef-a1b1-a63bbbf0d506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "Random example word: /c/de/aufmachung\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {}\".format(all_embeddings.shape))\n",
    "\n",
    "print(\"Random example word: {}\".format(all_words[1337]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38afa287-41ca-4e39-99d5-841b09741e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2,  -2, -15, ...,  -2,  -3,   0],\n",
       "       [ -1,  -1,  -2, ...,  -2,  -2,   2],\n",
       "       [  0,   0,  -2, ...,   0,   0,  -2],\n",
       "       ...,\n",
       "       [  0,   3,   4, ...,   0,   1,   2],\n",
       "       [ -2,   3,   3, ...,   0,  -7,   1],\n",
       "       [  1,   0,   2, ...,  -7,   0,   0]], dtype=int8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a24347-9bb3-4d57-9c19-299e62868440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/de/###er',\n",
       " '/c/de/##jahre',\n",
       " '/c/de/0',\n",
       " '/c/de/1',\n",
       " '/c/de/2',\n",
       " '/c/de/2d',\n",
       " '/c/de/3',\n",
       " '/c/de/3d',\n",
       " '/c/de/4',\n",
       " '/c/de/5',\n",
       " '/c/de/6',\n",
       " '/c/de/7',\n",
       " '/c/de/8',\n",
       " '/c/de/9',\n",
       " '/c/de/a',\n",
       " '/c/de/a.d',\n",
       " '/c/de/aa',\n",
       " '/c/de/aaa',\n",
       " '/c/de/aachen',\n",
       " '/c/de/aachener',\n",
       " '/c/de/aal',\n",
       " '/c/de/aalen',\n",
       " '/c/de/aarau',\n",
       " '/c/de/aargau',\n",
       " '/c/de/aaron',\n",
       " '/c/de/ab',\n",
       " '/c/de/aba',\n",
       " '/c/de/abarbeiten',\n",
       " '/c/de/abartig',\n",
       " '/c/de/abb',\n",
       " '/c/de/abba',\n",
       " '/c/de/abbas',\n",
       " '/c/de/abbau',\n",
       " '/c/de/abbauen',\n",
       " '/c/de/abbekommen',\n",
       " '/c/de/abbiegen',\n",
       " '/c/de/abbild',\n",
       " '/c/de/abbilden',\n",
       " '/c/de/abbildung',\n",
       " '/c/de/abbrechen',\n",
       " '/c/de/abbringen',\n",
       " '/c/de/abbruch',\n",
       " '/c/de/abc',\n",
       " '/c/de/abdecken',\n",
       " '/c/de/abdeckung',\n",
       " '/c/de/abdruck',\n",
       " '/c/de/abe',\n",
       " '/c/de/abel',\n",
       " '/c/de/abend',\n",
       " '/c/de/abendblatt',\n",
       " '/c/de/abendbrot',\n",
       " '/c/de/abendessen',\n",
       " '/c/de/abendland',\n",
       " '/c/de/abendmahl',\n",
       " '/c/de/abends',\n",
       " '/c/de/abendzeitung',\n",
       " '/c/de/abenteuer',\n",
       " '/c/de/abenteuern',\n",
       " '/c/de/abenteurer',\n",
       " '/c/de/aber',\n",
       " '/c/de/aberglaube',\n",
       " '/c/de/aberglauben',\n",
       " '/c/de/abermals',\n",
       " '/c/de/abfahren',\n",
       " '/c/de/abfahrt',\n",
       " '/c/de/abfall',\n",
       " '/c/de/abfangen',\n",
       " '/c/de/abfinden',\n",
       " '/c/de/abfindung',\n",
       " '/c/de/abflug',\n",
       " '/c/de/abfluss',\n",
       " '/c/de/abfolge',\n",
       " '/c/de/abfrage',\n",
       " '/c/de/abfuhr',\n",
       " '/c/de/abfälle',\n",
       " '/c/de/abg',\n",
       " '/c/de/abgabe',\n",
       " '/c/de/abgaben',\n",
       " '/c/de/abgang',\n",
       " '/c/de/abgebaut',\n",
       " '/c/de/abgeben',\n",
       " '/c/de/abgebildet',\n",
       " '/c/de/abgebrannt',\n",
       " '/c/de/abgebrochen',\n",
       " '/c/de/abgedeckt',\n",
       " '/c/de/abgedreht',\n",
       " '/c/de/abgefahren',\n",
       " '/c/de/abgeführt',\n",
       " '/c/de/abgegeben',\n",
       " '/c/de/abgegrenzt',\n",
       " '/c/de/abgehen',\n",
       " '/c/de/abgehoben',\n",
       " '/c/de/abgekürzt',\n",
       " '/c/de/abgelaufen',\n",
       " '/c/de/abgelehnt',\n",
       " '/c/de/abgeleitet',\n",
       " '/c/de/abgelenkt',\n",
       " '/c/de/abgelöst',\n",
       " '/c/de/abgeneigt',\n",
       " '/c/de/abgenommen',\n",
       " '/c/de/abgeordnete',\n",
       " '/c/de/abgeordnetenhaus',\n",
       " '/c/de/abgeordneter',\n",
       " '/c/de/abgerissen',\n",
       " '/c/de/abgerufen',\n",
       " '/c/de/abgerundet',\n",
       " '/c/de/abgesagt',\n",
       " '/c/de/abgeschlagen',\n",
       " '/c/de/abgeschlossen',\n",
       " '/c/de/abgesehen',\n",
       " '/c/de/abgesetzt',\n",
       " '/c/de/abgesichert',\n",
       " '/c/de/abgesperrt',\n",
       " '/c/de/abgesprochen',\n",
       " '/c/de/abgestellt',\n",
       " '/c/de/abgestimmt',\n",
       " '/c/de/abgetan',\n",
       " '/c/de/abgetragen',\n",
       " '/c/de/abgetrennt',\n",
       " '/c/de/abgetreten',\n",
       " '/c/de/abgewinnen',\n",
       " '/c/de/abgeworfen',\n",
       " '/c/de/abgezogen',\n",
       " '/c/de/abgrenzen',\n",
       " '/c/de/abgrenzung',\n",
       " '/c/de/abgrund',\n",
       " '/c/de/abhalten',\n",
       " '/c/de/abhanden',\n",
       " '/c/de/abhandlung',\n",
       " '/c/de/abhauen',\n",
       " '/c/de/abheben',\n",
       " '/c/de/abhilfe',\n",
       " '/c/de/abholen',\n",
       " '/c/de/abhängen',\n",
       " '/c/de/abhängig',\n",
       " '/c/de/abhängigkeit',\n",
       " '/c/de/abhören',\n",
       " '/c/de/abi',\n",
       " '/c/de/abitur',\n",
       " '/c/de/abkaufen',\n",
       " '/c/de/abkehr',\n",
       " '/c/de/abkommen',\n",
       " '/c/de/abkühlen',\n",
       " '/c/de/abkühlung',\n",
       " '/c/de/abkürzung',\n",
       " '/c/de/abkürzungen',\n",
       " '/c/de/abl',\n",
       " '/c/de/ablassen',\n",
       " '/c/de/ablauf',\n",
       " '/c/de/ablaufen',\n",
       " '/c/de/ableben',\n",
       " '/c/de/ablegen',\n",
       " '/c/de/ableger',\n",
       " '/c/de/ablehnen',\n",
       " '/c/de/ablehnend',\n",
       " '/c/de/ablehnung',\n",
       " '/c/de/ableiten',\n",
       " '/c/de/ableitung',\n",
       " '/c/de/ablenken',\n",
       " '/c/de/ablenkung',\n",
       " '/c/de/ablesen',\n",
       " '/c/de/abliefern',\n",
       " '/c/de/ablöse',\n",
       " '/c/de/ablösen',\n",
       " '/c/de/ablösung',\n",
       " '/c/de/abmachung',\n",
       " '/c/de/abmahnung',\n",
       " '/c/de/abnahme',\n",
       " '/c/de/abnehmen',\n",
       " '/c/de/abnehmer',\n",
       " '/c/de/abneigung',\n",
       " '/c/de/abo',\n",
       " '/c/de/abonnement',\n",
       " '/c/de/abonnenten',\n",
       " '/c/de/abonnieren',\n",
       " '/c/de/abonniert',\n",
       " '/c/de/abpfiff',\n",
       " '/c/de/abraham',\n",
       " '/c/de/abrechnung',\n",
       " '/c/de/abreise',\n",
       " '/c/de/abreissen',\n",
       " '/c/de/abriss',\n",
       " '/c/de/abruf',\n",
       " '/c/de/abrufbar',\n",
       " '/c/de/abrufen',\n",
       " '/c/de/abrupt',\n",
       " '/c/de/abrüstung',\n",
       " '/c/de/abs',\n",
       " '/c/de/absage',\n",
       " '/c/de/absagen',\n",
       " '/c/de/absatz',\n",
       " '/c/de/abschaffen',\n",
       " '/c/de/abschaffung',\n",
       " '/c/de/abschalten',\n",
       " '/c/de/abschaltung',\n",
       " '/c/de/abschaum',\n",
       " '/c/de/abscheu',\n",
       " '/c/de/abschiebung',\n",
       " '/c/de/abschied',\n",
       " '/c/de/abschiessen',\n",
       " '/c/de/abschliessen',\n",
       " '/c/de/abschliessend',\n",
       " '/c/de/abschluss',\n",
       " '/c/de/abschlussfeier',\n",
       " '/c/de/abschlussprüfung',\n",
       " '/c/de/abschneiden',\n",
       " '/c/de/abschnitt',\n",
       " '/c/de/abschnitte',\n",
       " '/c/de/abschnitten',\n",
       " '/c/de/abschnitts',\n",
       " '/c/de/abschrecken',\n",
       " '/c/de/abschreckung',\n",
       " '/c/de/abschreiben',\n",
       " '/c/de/abschuss',\n",
       " '/c/de/abschätzen',\n",
       " '/c/de/absehbar',\n",
       " '/c/de/absehen',\n",
       " '/c/de/abseits',\n",
       " '/c/de/absender',\n",
       " '/c/de/absenkung',\n",
       " '/c/de/absetzen',\n",
       " '/c/de/absetzung',\n",
       " '/c/de/absichern',\n",
       " '/c/de/absicherung',\n",
       " '/c/de/absicht',\n",
       " '/c/de/absichten',\n",
       " '/c/de/absichtlich',\n",
       " '/c/de/absolut',\n",
       " '/c/de/absolute',\n",
       " '/c/de/absolutes',\n",
       " '/c/de/absolvent',\n",
       " '/c/de/absolvieren',\n",
       " '/c/de/absolviert',\n",
       " '/c/de/abspaltung',\n",
       " '/c/de/abspann',\n",
       " '/c/de/absperrung',\n",
       " '/c/de/abspielen',\n",
       " '/c/de/absprache',\n",
       " '/c/de/absprechen',\n",
       " '/c/de/abstammung',\n",
       " '/c/de/abstand',\n",
       " '/c/de/abstecher',\n",
       " '/c/de/absteigen',\n",
       " '/c/de/absteiger',\n",
       " '/c/de/abstellen',\n",
       " '/c/de/abstieg',\n",
       " '/c/de/abstiegskampf',\n",
       " '/c/de/abstimmen',\n",
       " '/c/de/abstimmung',\n",
       " '/c/de/abstinenz',\n",
       " '/c/de/abstossend',\n",
       " '/c/de/abstract',\n",
       " '/c/de/abstrakt',\n",
       " '/c/de/abstraktion',\n",
       " '/c/de/absturz',\n",
       " '/c/de/abständen',\n",
       " '/c/de/abstürzen',\n",
       " '/c/de/absurd',\n",
       " '/c/de/abt',\n",
       " '/c/de/abtei',\n",
       " '/c/de/abteil',\n",
       " '/c/de/abteilung',\n",
       " '/c/de/abteilungsleiter',\n",
       " '/c/de/abtransport',\n",
       " '/c/de/abtreibung',\n",
       " '/c/de/abtreten',\n",
       " '/c/de/abwanderung',\n",
       " '/c/de/abwarten',\n",
       " '/c/de/abwasser',\n",
       " '/c/de/abwechselnd',\n",
       " '/c/de/abwechslung',\n",
       " '/c/de/abwechslungsreich',\n",
       " '/c/de/abwegig',\n",
       " '/c/de/abwehr',\n",
       " '/c/de/abwehren',\n",
       " '/c/de/abwehrspieler',\n",
       " '/c/de/abweichen',\n",
       " '/c/de/abweichend',\n",
       " '/c/de/abweichung',\n",
       " '/c/de/abwenden',\n",
       " '/c/de/abwerfen',\n",
       " '/c/de/abwertung',\n",
       " '/c/de/abwesend',\n",
       " '/c/de/abwesenheit',\n",
       " '/c/de/abwicklung',\n",
       " '/c/de/abwägen',\n",
       " '/c/de/abwägung',\n",
       " '/c/de/abwärts',\n",
       " '/c/de/abzeichen',\n",
       " '/c/de/abziehen',\n",
       " '/c/de/abzielen',\n",
       " '/c/de/abzocke',\n",
       " '/c/de/abzug',\n",
       " '/c/de/abzweigung',\n",
       " '/c/de/abzüge',\n",
       " '/c/de/abzüglich',\n",
       " '/c/de/abänderung',\n",
       " '/c/de/ac',\n",
       " '/c/de/access',\n",
       " '/c/de/accessoires',\n",
       " '/c/de/account',\n",
       " '/c/de/ace',\n",
       " '/c/de/ach',\n",
       " '/c/de/achilles',\n",
       " '/c/de/achim',\n",
       " '/c/de/achse',\n",
       " '/c/de/achsen',\n",
       " '/c/de/acht',\n",
       " '/c/de/achte',\n",
       " '/c/de/achtelfinale',\n",
       " '/c/de/achten',\n",
       " '/c/de/achter',\n",
       " '/c/de/achterbahn',\n",
       " '/c/de/achtsamkeit',\n",
       " '/c/de/achtung',\n",
       " '/c/de/achtzehn',\n",
       " '/c/de/achtzig',\n",
       " '/c/de/achtziger',\n",
       " '/c/de/acid',\n",
       " '/c/de/acker',\n",
       " '/c/de/ackerbau',\n",
       " '/c/de/ackerland',\n",
       " '/c/de/ackermann',\n",
       " '/c/de/action',\n",
       " '/c/de/ad',\n",
       " '/c/de/ada',\n",
       " '/c/de/adac',\n",
       " '/c/de/adalbert',\n",
       " '/c/de/adam',\n",
       " '/c/de/adapter',\n",
       " '/c/de/adaption',\n",
       " '/c/de/add',\n",
       " '/c/de/addiert',\n",
       " '/c/de/ade',\n",
       " '/c/de/adel',\n",
       " '/c/de/adele',\n",
       " '/c/de/adenauer',\n",
       " '/c/de/ader',\n",
       " '/c/de/adern',\n",
       " '/c/de/adhs',\n",
       " '/c/de/adieu',\n",
       " '/c/de/adjektiv',\n",
       " '/c/de/adjektive',\n",
       " '/c/de/adler',\n",
       " '/c/de/admin',\n",
       " '/c/de/administration',\n",
       " '/c/de/administrative',\n",
       " '/c/de/administrator',\n",
       " '/c/de/admiral',\n",
       " '/c/de/adobe',\n",
       " '/c/de/adolf',\n",
       " '/c/de/adoptieren',\n",
       " '/c/de/adoption',\n",
       " '/c/de/adrenalin',\n",
       " '/c/de/adresse',\n",
       " '/c/de/adria',\n",
       " '/c/de/adrian',\n",
       " '/c/de/adult',\n",
       " '/c/de/advent',\n",
       " '/c/de/adäquat',\n",
       " '/c/de/ae',\n",
       " '/c/de/af',\n",
       " '/c/de/affe',\n",
       " '/c/de/affen',\n",
       " '/c/de/affinität',\n",
       " '/c/de/affäre',\n",
       " '/c/de/affären',\n",
       " '/c/de/afghanistan',\n",
       " '/c/de/afrika',\n",
       " '/c/de/afrikaner',\n",
       " '/c/de/afrikas',\n",
       " '/c/de/after',\n",
       " '/c/de/ag',\n",
       " '/c/de/agb',\n",
       " '/c/de/age',\n",
       " '/c/de/agency',\n",
       " '/c/de/agenda',\n",
       " '/c/de/agent',\n",
       " '/c/de/agentur',\n",
       " '/c/de/aggression',\n",
       " '/c/de/aggressiv',\n",
       " '/c/de/aggressivität',\n",
       " '/c/de/agieren',\n",
       " '/c/de/agitation',\n",
       " '/c/de/agnes',\n",
       " '/c/de/agrar',\n",
       " '/c/de/agrarpolitik',\n",
       " '/c/de/ah',\n",
       " '/c/de/aha',\n",
       " '/c/de/ahaus',\n",
       " '/c/de/ahmad',\n",
       " '/c/de/ahnen',\n",
       " '/c/de/ahnt',\n",
       " '/c/de/ahnte',\n",
       " '/c/de/ahnung',\n",
       " '/c/de/ahnungslos',\n",
       " '/c/de/ai',\n",
       " '/c/de/aids',\n",
       " '/c/de/ain',\n",
       " '/c/de/air',\n",
       " '/c/de/airbus',\n",
       " '/c/de/airline',\n",
       " '/c/de/airport',\n",
       " '/c/de/ajax',\n",
       " '/c/de/ak',\n",
       " '/c/de/aka',\n",
       " '/c/de/akademie',\n",
       " '/c/de/akademiker',\n",
       " '/c/de/akademisch',\n",
       " '/c/de/akkord',\n",
       " '/c/de/akkordeon',\n",
       " '/c/de/akku',\n",
       " '/c/de/akkumulation',\n",
       " '/c/de/akt',\n",
       " '/c/de/akte',\n",
       " '/c/de/akten',\n",
       " '/c/de/aktenzeichen',\n",
       " '/c/de/akteur',\n",
       " '/c/de/aktie',\n",
       " '/c/de/aktien',\n",
       " '/c/de/aktiengesellschaft',\n",
       " '/c/de/aktienmarkt',\n",
       " '/c/de/aktion',\n",
       " '/c/de/aktionsplan',\n",
       " '/c/de/aktionstag',\n",
       " '/c/de/aktiv',\n",
       " '/c/de/aktive',\n",
       " '/c/de/aktiven',\n",
       " '/c/de/aktiver',\n",
       " '/c/de/aktives',\n",
       " '/c/de/aktivieren',\n",
       " '/c/de/aktiviert',\n",
       " '/c/de/aktivierung',\n",
       " '/c/de/aktivist',\n",
       " '/c/de/aktivität',\n",
       " '/c/de/aktivitäten',\n",
       " '/c/de/aktualisieren',\n",
       " '/c/de/aktualisiert',\n",
       " '/c/de/aktualisierung',\n",
       " '/c/de/aktualität',\n",
       " '/c/de/aktuell',\n",
       " '/c/de/akustik',\n",
       " '/c/de/akustisch',\n",
       " '/c/de/akut',\n",
       " '/c/de/akute',\n",
       " '/c/de/akuten',\n",
       " '/c/de/akw',\n",
       " '/c/de/akzent',\n",
       " '/c/de/akzeptabel',\n",
       " '/c/de/akzeptanz',\n",
       " '/c/de/akzeptieren',\n",
       " '/c/de/akzeptiert',\n",
       " '/c/de/al',\n",
       " '/c/de/ala',\n",
       " '/c/de/alabama',\n",
       " '/c/de/alan',\n",
       " '/c/de/alarm',\n",
       " '/c/de/alaska',\n",
       " '/c/de/alb',\n",
       " '/c/de/alba',\n",
       " '/c/de/albaner',\n",
       " '/c/de/albanien',\n",
       " '/c/de/alben',\n",
       " '/c/de/albern',\n",
       " '/c/de/albert',\n",
       " '/c/de/albrecht',\n",
       " '/c/de/albtraum',\n",
       " '/c/de/album',\n",
       " '/c/de/ale',\n",
       " '/c/de/aleppo',\n",
       " '/c/de/alex',\n",
       " '/c/de/alexander',\n",
       " '/c/de/alexandra',\n",
       " '/c/de/alexandria',\n",
       " '/c/de/alexis',\n",
       " '/c/de/alf',\n",
       " '/c/de/alfons',\n",
       " '/c/de/alfred',\n",
       " '/c/de/algebra',\n",
       " '/c/de/algen',\n",
       " '/c/de/algerien',\n",
       " '/c/de/algier',\n",
       " '/c/de/algorithmus',\n",
       " '/c/de/ali',\n",
       " '/c/de/alias',\n",
       " '/c/de/alibi',\n",
       " '/c/de/alice',\n",
       " '/c/de/alien',\n",
       " '/c/de/alina',\n",
       " '/c/de/alk',\n",
       " '/c/de/alkohol',\n",
       " '/c/de/alkoholiker',\n",
       " '/c/de/alkoholismus',\n",
       " '/c/de/alkoholkonsum',\n",
       " '/c/de/all',\n",
       " '/c/de/allah',\n",
       " '/c/de/alle',\n",
       " '/c/de/allee',\n",
       " '/c/de/allein',\n",
       " '/c/de/alleine',\n",
       " '/c/de/alleingang',\n",
       " '/c/de/allem',\n",
       " '/c/de/allemal',\n",
       " '/c/de/allen',\n",
       " '/c/de/allenfalls',\n",
       " '/c/de/allenthalben',\n",
       " '/c/de/aller',\n",
       " '/c/de/allerdings',\n",
       " '/c/de/allergie',\n",
       " '/c/de/allergien',\n",
       " '/c/de/allergisch',\n",
       " '/c/de/allerhand',\n",
       " '/c/de/allerlei',\n",
       " '/c/de/allerseits',\n",
       " '/c/de/alles',\n",
       " '/c/de/allesamt',\n",
       " '/c/de/allgegenwärtig',\n",
       " '/c/de/allgemein',\n",
       " '/c/de/allgemeine',\n",
       " '/c/de/allgemeines',\n",
       " '/c/de/allgemeinheit',\n",
       " '/c/de/allgäu',\n",
       " '/c/de/allianz',\n",
       " '/c/de/alliierte',\n",
       " '/c/de/alljährlich',\n",
       " '/c/de/allmacht',\n",
       " '/c/de/allmählich',\n",
       " '/c/de/allseits',\n",
       " '/c/de/alltag',\n",
       " '/c/de/alltags',\n",
       " '/c/de/alltagsleben',\n",
       " '/c/de/alltäglich',\n",
       " '/c/de/allzu',\n",
       " '/c/de/alm',\n",
       " '/c/de/alma',\n",
       " '/c/de/almosen',\n",
       " '/c/de/alois',\n",
       " '/c/de/alpen',\n",
       " '/c/de/alpha',\n",
       " '/c/de/alphabet',\n",
       " '/c/de/alpin',\n",
       " '/c/de/alptraum',\n",
       " '/c/de/als',\n",
       " '/c/de/alsbald',\n",
       " '/c/de/also',\n",
       " '/c/de/alt',\n",
       " '/c/de/altar',\n",
       " '/c/de/altbau',\n",
       " '/c/de/altdorf',\n",
       " '/c/de/alte',\n",
       " '/c/de/alten',\n",
       " '/c/de/altenburg',\n",
       " '/c/de/altenheim',\n",
       " '/c/de/alter',\n",
       " '/c/de/altern',\n",
       " '/c/de/alternativ',\n",
       " '/c/de/alternative',\n",
       " '/c/de/alternativen',\n",
       " '/c/de/altersgrenze',\n",
       " '/c/de/altersgruppe',\n",
       " '/c/de/altersheim',\n",
       " '/c/de/altertum',\n",
       " '/c/de/altes',\n",
       " '/c/de/altmark',\n",
       " '/c/de/altmodisch',\n",
       " '/c/de/altona',\n",
       " '/c/de/altstadt',\n",
       " '/c/de/alu',\n",
       " '/c/de/aluminium',\n",
       " '/c/de/alwin',\n",
       " '/c/de/alzheimer',\n",
       " '/c/de/am',\n",
       " '/c/de/ama',\n",
       " '/c/de/amadeus',\n",
       " '/c/de/amanda',\n",
       " '/c/de/amateur',\n",
       " '/c/de/amazon',\n",
       " '/c/de/amazonas',\n",
       " '/c/de/amberg',\n",
       " '/c/de/ambiente',\n",
       " '/c/de/ambitionen',\n",
       " '/c/de/ambivalenz',\n",
       " '/c/de/ambulante',\n",
       " '/c/de/ambulanz',\n",
       " '/c/de/amd',\n",
       " '/c/de/ameisen',\n",
       " '/c/de/amen',\n",
       " '/c/de/amerika',\n",
       " '/c/de/amerikaner',\n",
       " '/c/de/amerikanerin',\n",
       " '/c/de/amerikanisch',\n",
       " '/c/de/ami',\n",
       " '/c/de/amiga',\n",
       " '/c/de/amis',\n",
       " '/c/de/ammoniak',\n",
       " '/c/de/amnestie',\n",
       " '/c/de/amok',\n",
       " '/c/de/amoklauf',\n",
       " '/c/de/amor',\n",
       " '/c/de/amos',\n",
       " '/c/de/ampel',\n",
       " '/c/de/amphibien',\n",
       " '/c/de/amsterdam',\n",
       " '/c/de/amsterdamer',\n",
       " '/c/de/amt',\n",
       " '/c/de/amtlich',\n",
       " '/c/de/amts',\n",
       " '/c/de/amtsantritt',\n",
       " '/c/de/amtsblatt',\n",
       " '/c/de/amtsgericht',\n",
       " '/c/de/amtsinhaber',\n",
       " '/c/de/amtszeit',\n",
       " '/c/de/amüsant',\n",
       " '/c/de/amüsieren',\n",
       " '/c/de/amüsiert',\n",
       " '/c/de/an',\n",
       " '/c/de/ana',\n",
       " '/c/de/anal',\n",
       " '/c/de/analog',\n",
       " '/c/de/analogie',\n",
       " '/c/de/analphabeten',\n",
       " '/c/de/analyse',\n",
       " '/c/de/analysieren',\n",
       " '/c/de/analysiert',\n",
       " '/c/de/analysis',\n",
       " '/c/de/ananas',\n",
       " '/c/de/anarchie',\n",
       " '/c/de/anarchismus',\n",
       " '/c/de/anastasia',\n",
       " '/c/de/anatomie',\n",
       " '/c/de/anbau',\n",
       " '/c/de/anbauen',\n",
       " '/c/de/anbeginn',\n",
       " '/c/de/anbetung',\n",
       " '/c/de/anbieten',\n",
       " '/c/de/anbieter',\n",
       " '/c/de/anbietet',\n",
       " '/c/de/anbindung',\n",
       " '/c/de/anblick',\n",
       " '/c/de/anbringen',\n",
       " '/c/de/and',\n",
       " '/c/de/andacht',\n",
       " '/c/de/andauern',\n",
       " '/c/de/andauernd',\n",
       " '/c/de/anden',\n",
       " '/c/de/andenken',\n",
       " '/c/de/ander',\n",
       " '/c/de/andere',\n",
       " '/c/de/anderem',\n",
       " '/c/de/anderen',\n",
       " '/c/de/anderer',\n",
       " '/c/de/andererseits',\n",
       " '/c/de/anderes',\n",
       " '/c/de/andernfalls',\n",
       " '/c/de/andernorts',\n",
       " '/c/de/anders',\n",
       " '/c/de/anderseits',\n",
       " '/c/de/andersrum',\n",
       " '/c/de/anderswo',\n",
       " '/c/de/anderthalb',\n",
       " '/c/de/anderweitig',\n",
       " '/c/de/andeuten',\n",
       " '/c/de/andorra',\n",
       " '/c/de/andrang',\n",
       " '/c/de/andre',\n",
       " '/c/de/andrea',\n",
       " '/c/de/andreas',\n",
       " '/c/de/androhung',\n",
       " '/c/de/android',\n",
       " '/c/de/aneignen',\n",
       " '/c/de/aneignung',\n",
       " '/c/de/aneinander',\n",
       " '/c/de/anekdote',\n",
       " '/c/de/anerkannt',\n",
       " '/c/de/anerkennen',\n",
       " '/c/de/anerkennung',\n",
       " '/c/de/anfall',\n",
       " '/c/de/anfallen',\n",
       " '/c/de/anfang',\n",
       " '/c/de/anfange',\n",
       " '/c/de/anfangen',\n",
       " '/c/de/anfangs',\n",
       " '/c/de/anfangsphase',\n",
       " '/c/de/anfangszeit',\n",
       " '/c/de/anfassen',\n",
       " '/c/de/anfertigen',\n",
       " '/c/de/anflug',\n",
       " '/c/de/anfordern',\n",
       " '/c/de/anforderung',\n",
       " '/c/de/anforderungen',\n",
       " '/c/de/anfrage',\n",
       " '/c/de/anfragen',\n",
       " '/c/de/anfreunden',\n",
       " '/c/de/anfällig',\n",
       " '/c/de/anfänger',\n",
       " '/c/de/anfänglich',\n",
       " '/c/de/anführen',\n",
       " '/c/de/anführer',\n",
       " '/c/de/anführungszeichen',\n",
       " '/c/de/ang',\n",
       " '/c/de/angabe',\n",
       " '/c/de/angaben',\n",
       " '/c/de/angebaut',\n",
       " '/c/de/angeben',\n",
       " '/c/de/angeblich',\n",
       " '/c/de/angebot',\n",
       " '/c/de/angeboten',\n",
       " '/c/de/angebracht',\n",
       " '/c/de/angebunden',\n",
       " '/c/de/angedacht',\n",
       " '/c/de/angedeutet',\n",
       " '/c/de/angeeignet',\n",
       " '/c/de/angefasst',\n",
       " '/c/de/angefertigt',\n",
       " '/c/de/angeführt',\n",
       " '/c/de/angegangen',\n",
       " '/c/de/angegeben',\n",
       " '/c/de/angegriffen',\n",
       " '/c/de/angehen',\n",
       " '/c/de/angehoben',\n",
       " '/c/de/angehängt',\n",
       " '/c/de/angehören',\n",
       " '/c/de/angehörige',\n",
       " '/c/de/angehöriger',\n",
       " '/c/de/angehört',\n",
       " '/c/de/angeklagt',\n",
       " '/c/de/angeklagte',\n",
       " '/c/de/angekommen',\n",
       " '/c/de/angekündigt',\n",
       " '/c/de/angel',\n",
       " '/c/de/angela',\n",
       " '/c/de/angelaufen',\n",
       " '/c/de/angelegenheit',\n",
       " '/c/de/angelegenheiten',\n",
       " '/c/de/angelegt',\n",
       " '/c/de/angelehnt',\n",
       " '/c/de/angelika',\n",
       " '/c/de/angeln',\n",
       " '/c/de/angelo',\n",
       " '/c/de/angemeldet',\n",
       " '/c/de/angemessen',\n",
       " '/c/de/angenehm',\n",
       " '/c/de/angenommen',\n",
       " '/c/de/angeordnet',\n",
       " '/c/de/angepasst',\n",
       " '/c/de/angepisst',\n",
       " '/c/de/anger',\n",
       " '/c/de/angeregt',\n",
       " '/c/de/angereist',\n",
       " '/c/de/angesagt',\n",
       " '/c/de/angeschlagen',\n",
       " '/c/de/angeschlossen',\n",
       " '/c/de/angeschossen',\n",
       " '/c/de/angesehen',\n",
       " '/c/de/angesicht',\n",
       " '/c/de/angesichts',\n",
       " '/c/de/angesiedelt',\n",
       " '/c/de/angespannt',\n",
       " '/c/de/angestellt',\n",
       " '/c/de/angestellte',\n",
       " '/c/de/angestellten',\n",
       " '/c/de/angestellter',\n",
       " '/c/de/angestiegen',\n",
       " '/c/de/angestrebt',\n",
       " '/c/de/angestrengt',\n",
       " '/c/de/angetan',\n",
       " '/c/de/angetreten',\n",
       " '/c/de/angetrieben',\n",
       " '/c/de/angewandt',\n",
       " '/c/de/angewendet',\n",
       " '/c/de/angewiesen',\n",
       " '/c/de/angezeigt',\n",
       " '/c/de/angezogen',\n",
       " '/c/de/angezweifelt',\n",
       " '/c/de/angezündet',\n",
       " '/c/de/angleichung',\n",
       " '/c/de/anglo',\n",
       " '/c/de/angola',\n",
       " '/c/de/angreifen',\n",
       " '/c/de/angreifer',\n",
       " '/c/de/angriff',\n",
       " '/c/de/angriffe',\n",
       " '/c/de/angriffen',\n",
       " '/c/de/angriffs',\n",
       " '/c/de/angst',\n",
       " '/c/de/angucken',\n",
       " '/c/de/anhaben',\n",
       " '/c/de/anhalt',\n",
       " '/c/de/anhalten',\n",
       " '/c/de/anhaltend',\n",
       " '/c/de/anhalter',\n",
       " '/c/de/anhaltspunkt',\n",
       " '/c/de/anhand',\n",
       " '/c/de/anhang',\n",
       " '/c/de/anheben',\n",
       " '/c/de/anhebung',\n",
       " '/c/de/anhieb',\n",
       " '/c/de/anhängen',\n",
       " '/c/de/anhänger',\n",
       " '/c/de/anhören',\n",
       " '/c/de/anhörung',\n",
       " '/c/de/animation',\n",
       " '/c/de/anime',\n",
       " '/c/de/animieren',\n",
       " '/c/de/animiert',\n",
       " '/c/de/anja',\n",
       " '/c/de/ankara',\n",
       " '/c/de/ankauf',\n",
       " '/c/de/anke',\n",
       " '/c/de/anker',\n",
       " '/c/de/anklage',\n",
       " '/c/de/anklagen',\n",
       " '/c/de/anklang',\n",
       " '/c/de/anklicken',\n",
       " '/c/de/ankläger',\n",
       " '/c/de/anknüpfen',\n",
       " '/c/de/ankommen',\n",
       " '/c/de/ankunft',\n",
       " '/c/de/ankündigen',\n",
       " '/c/de/ankündigung',\n",
       " '/c/de/anlage',\n",
       " '/c/de/anlagen',\n",
       " '/c/de/anlass',\n",
       " '/c/de/anlauf',\n",
       " '/c/de/anlaufstelle',\n",
       " '/c/de/anlegen',\n",
       " '/c/de/anleger',\n",
       " '/c/de/anlehnung',\n",
       " '/c/de/anleihe',\n",
       " '/c/de/anleihen',\n",
       " '/c/de/anleitung',\n",
       " '/c/de/anliegen',\n",
       " '/c/de/anlocken',\n",
       " '/c/de/anlässen',\n",
       " '/c/de/anlässlich',\n",
       " '/c/de/anmachen',\n",
       " '/c/de/anmelden',\n",
       " '/c/de/anmeldung',\n",
       " '/c/de/anmerken',\n",
       " '/c/de/anmerkung',\n",
       " '/c/de/anmut',\n",
       " '/c/de/ann',\n",
       " '/c/de/anna',\n",
       " '/c/de/annahme',\n",
       " '/c/de/annahmen',\n",
       " '/c/de/annalen',\n",
       " '/c/de/anne',\n",
       " '/c/de/annehmen',\n",
       " '/c/de/annemarie',\n",
       " '/c/de/annexion',\n",
       " '/c/de/anno',\n",
       " '/c/de/annähernd',\n",
       " '/c/de/annäherung',\n",
       " '/c/de/anomalie',\n",
       " '/c/de/anonym',\n",
       " '/c/de/anonymität',\n",
       " '/c/de/anordnung',\n",
       " '/c/de/anpacken',\n",
       " '/c/de/anpassen',\n",
       " '/c/de/anpassung',\n",
       " '/c/de/anpfiff',\n",
       " '/c/de/anrechnung',\n",
       " '/c/de/anrecht',\n",
       " '/c/de/anrede',\n",
       " '/c/de/anregen',\n",
       " '/c/de/anregung',\n",
       " '/c/de/anreise',\n",
       " '/c/de/anreiz',\n",
       " '/c/de/anrichten',\n",
       " '/c/de/anruf',\n",
       " '/c/de/anrufe',\n",
       " '/c/de/anrufen',\n",
       " '/c/de/anrufer',\n",
       " '/c/de/ans',\n",
       " '/c/de/ansage',\n",
       " '/c/de/ansagen',\n",
       " '/c/de/ansammlung',\n",
       " '/c/de/ansatz',\n",
       " '/c/de/ansatzweise',\n",
       " '/c/de/ansbach',\n",
       " '/c/de/anschaffen',\n",
       " '/c/de/anschaffung',\n",
       " '/c/de/anschauen',\n",
       " '/c/de/anschaulich',\n",
       " '/c/de/anschauung',\n",
       " '/c/de/anschauungen',\n",
       " '/c/de/anschein',\n",
       " '/c/de/anscheinend',\n",
       " '/c/de/anschlag',\n",
       " '/c/de/anschliessen',\n",
       " '/c/de/anschliessend',\n",
       " '/c/de/anschliessende',\n",
       " '/c/de/anschluss',\n",
       " '/c/de/anschlussstelle',\n",
       " '/c/de/anschreiben',\n",
       " '/c/de/anschrift',\n",
       " '/c/de/anschuldigung',\n",
       " '/c/de/ansehen',\n",
       " '/c/de/ansetzen',\n",
       " '/c/de/ansgar',\n",
       " '/c/de/ansicht',\n",
       " '/c/de/ansichten',\n",
       " '/c/de/ansiedlung',\n",
       " '/c/de/ansinnen',\n",
       " '/c/de/ansonsten',\n",
       " '/c/de/anspannung',\n",
       " '/c/de/anspielung',\n",
       " '/c/de/ansporn',\n",
       " '/c/de/ansprache',\n",
       " '/c/de/ansprechen',\n",
       " '/c/de/ansprechend',\n",
       " '/c/de/ansprechpartner',\n",
       " '/c/de/anspruch',\n",
       " '/c/de/anspruchsvoll',\n",
       " '/c/de/anstalt',\n",
       " '/c/de/anstand',\n",
       " '/c/de/anstatt',\n",
       " '/c/de/anstecken',\n",
       " '/c/de/ansteckend',\n",
       " '/c/de/anstehen',\n",
       " '/c/de/ansteigen',\n",
       " '/c/de/anstelle',\n",
       " '/c/de/anstellen',\n",
       " '/c/de/anstellung',\n",
       " '/c/de/anstieg',\n",
       " '/c/de/anstiftung',\n",
       " '/c/de/anstoss',\n",
       " '/c/de/anstossen',\n",
       " '/c/de/anstreben',\n",
       " '/c/de/anstrengen',\n",
       " '/c/de/anstrengend',\n",
       " '/c/de/anstrengung',\n",
       " '/c/de/anstrich',\n",
       " '/c/de/ansturm',\n",
       " '/c/de/anständig',\n",
       " '/c/de/ansässig',\n",
       " '/c/de/ant',\n",
       " '/c/de/antarktis',\n",
       " '/c/de/ante',\n",
       " '/c/de/anteil',\n",
       " '/c/de/anteilnahme',\n",
       " '/c/de/antenne',\n",
       " '/c/de/antennen',\n",
       " '/c/de/anthropologie',\n",
       " '/c/de/anti',\n",
       " '/c/de/antibiotika',\n",
       " '/c/de/antifa',\n",
       " '/c/de/antifaschismus',\n",
       " '/c/de/antike',\n",
       " '/c/de/antiken',\n",
       " '/c/de/antiker',\n",
       " '/c/de/antikörper',\n",
       " '/c/de/antiquitäten',\n",
       " '/c/de/antisemitisch',\n",
       " '/c/de/antisemitismus',\n",
       " '/c/de/antlitz',\n",
       " '/c/de/anton',\n",
       " '/c/de/antonia',\n",
       " '/c/de/antonius',\n",
       " '/c/de/antrag',\n",
       " '/c/de/antragsteller',\n",
       " '/c/de/antreffen',\n",
       " '/c/de/antreten',\n",
       " '/c/de/antrieb',\n",
       " '/c/de/antriebe',\n",
       " '/c/de/antritt',\n",
       " '/c/de/antun',\n",
       " '/c/de/antwerpen',\n",
       " '/c/de/antwort',\n",
       " '/c/de/antworte',\n",
       " '/c/de/antworten',\n",
       " '/c/de/antwortest',\n",
       " '/c/de/antwortet',\n",
       " '/c/de/antwortete',\n",
       " '/c/de/anus',\n",
       " '/c/de/anvertrauen',\n",
       " '/c/de/anvertraut',\n",
       " '/c/de/anwachsen',\n",
       " '/c/de/anwalt',\n",
       " '/c/de/anweisung',\n",
       " '/c/de/anweisungen',\n",
       " '/c/de/anwendbar',\n",
       " '/c/de/anwenden',\n",
       " '/c/de/anwender',\n",
       " '/c/de/anwendung',\n",
       " '/c/de/anwendungsbereich',\n",
       " '/c/de/anwesen',\n",
       " '/c/de/anwesend',\n",
       " '/c/de/anwesende',\n",
       " '/c/de/anwesenheit',\n",
       " '/c/de/anwohner',\n",
       " '/c/de/anwältin',\n",
       " '/c/de/anwärter',\n",
       " '/c/de/anyone',\n",
       " '/c/de/anzahl',\n",
       " '/c/de/anzeichen',\n",
       " '/c/de/anzeige',\n",
       " '/c/de/anzeigen',\n",
       " '/c/de/anzeiger',\n",
       " '/c/de/anziehen',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5f8ab-c259-495f-8e7a-0452b91c1826",
   "metadata": {},
   "source": [
    "Picking just the english words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748853bf-2310-4fe5-a22c-9e98da28e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in all_words: 150875\n",
      "english_embeddings dimensions: (150875, 300)\n"
     ]
    }
   ],
   "source": [
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"Number of English words in all_words: {0}\".format(len(english_words)))\n",
    "print(\"english_embeddings dimensions: {0}\".format(english_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c626de-7536-4aa7-9edb-d2aa5e5b2087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#####ish',\n",
       " '####ed',\n",
       " '####s',\n",
       " '####th',\n",
       " '###.##e',\n",
       " '###.##i',\n",
       " '###er',\n",
       " '###s',\n",
       " '###st',\n",
       " '###th',\n",
       " '###z',\n",
       " '##base#',\n",
       " '##er',\n",
       " '##mo',\n",
       " '##nd',\n",
       " '##rd',\n",
       " '##s',\n",
       " '##st',\n",
       " '##th',\n",
       " '##x',\n",
       " '##º',\n",
       " '0',\n",
       " '0f',\n",
       " '0ff',\n",
       " '0h',\n",
       " '0k',\n",
       " '0ld',\n",
       " '0n',\n",
       " '0ne',\n",
       " '0r',\n",
       " '0s',\n",
       " '0th',\n",
       " '0ver',\n",
       " '1',\n",
       " '1_corinthians',\n",
       " '1a',\n",
       " '1am',\n",
       " '1b',\n",
       " '1d',\n",
       " '1g',\n",
       " '1m',\n",
       " '1mm',\n",
       " '1o',\n",
       " '1pm',\n",
       " '1pp',\n",
       " '1s',\n",
       " '1st',\n",
       " '1th',\n",
       " '2',\n",
       " '2.0',\n",
       " '2_2_2',\n",
       " '2a',\n",
       " '2am',\n",
       " '2b',\n",
       " '2c',\n",
       " '2cv',\n",
       " '2d',\n",
       " '2dr',\n",
       " '2f',\n",
       " '2g',\n",
       " '2k',\n",
       " '2km',\n",
       " '2l',\n",
       " '2lt',\n",
       " '2m',\n",
       " '2nd',\n",
       " '2o',\n",
       " '2ot',\n",
       " '2pac',\n",
       " '2pm',\n",
       " '2s',\n",
       " '2th',\n",
       " '2wd',\n",
       " '2x',\n",
       " '2x4',\n",
       " '3',\n",
       " '3_d',\n",
       " '3a',\n",
       " '3am',\n",
       " '3b',\n",
       " '3c',\n",
       " '3d',\n",
       " '3d_printer',\n",
       " '3f',\n",
       " '3g',\n",
       " '3k',\n",
       " '3m',\n",
       " '3o',\n",
       " '3ot',\n",
       " '3p',\n",
       " '3pm',\n",
       " '3po',\n",
       " '3rd',\n",
       " '3s',\n",
       " '3th',\n",
       " '3w',\n",
       " '4',\n",
       " '4_aminopyridine',\n",
       " '4_h_er',\n",
       " '4a',\n",
       " '4am',\n",
       " '4b',\n",
       " '4c',\n",
       " '4chan',\n",
       " '4d',\n",
       " '4d_ultrasound',\n",
       " '4dr',\n",
       " '4eva',\n",
       " '4ever',\n",
       " '4f',\n",
       " '4g',\n",
       " '4gl',\n",
       " '4h',\n",
       " '4k',\n",
       " '4o',\n",
       " '4pm',\n",
       " '4s',\n",
       " '4th',\n",
       " '4to',\n",
       " '4wd',\n",
       " '4x2',\n",
       " '4x4',\n",
       " '4º',\n",
       " '5',\n",
       " '5a',\n",
       " '5am',\n",
       " '5b',\n",
       " '5c',\n",
       " '5d',\n",
       " '5ever',\n",
       " '5ft',\n",
       " '5g',\n",
       " '5k',\n",
       " '5km',\n",
       " '5m',\n",
       " '5o',\n",
       " '5pm',\n",
       " '5s',\n",
       " '5th',\n",
       " '6',\n",
       " '6a',\n",
       " '6am',\n",
       " '6b',\n",
       " '6ft',\n",
       " '6h',\n",
       " '6mo',\n",
       " '6o',\n",
       " '6pm',\n",
       " '6s',\n",
       " '6th',\n",
       " '6x6',\n",
       " '7',\n",
       " '7a',\n",
       " '7am',\n",
       " '7b',\n",
       " '7g',\n",
       " '7pm',\n",
       " '7s',\n",
       " '7th',\n",
       " '7up',\n",
       " '8',\n",
       " '8.3',\n",
       " '8a',\n",
       " '8am',\n",
       " '8b',\n",
       " '8c',\n",
       " '8mm',\n",
       " '8pm',\n",
       " '8s',\n",
       " '8th',\n",
       " '8va',\n",
       " '8vo',\n",
       " '8x8',\n",
       " '9',\n",
       " '9a',\n",
       " '9am',\n",
       " '9b',\n",
       " '9c',\n",
       " '9f',\n",
       " '9mm',\n",
       " '9mm_pistol',\n",
       " '9pm',\n",
       " '9s',\n",
       " '9th',\n",
       " 'a',\n",
       " 'a##',\n",
       " 'a###',\n",
       " 'a##y',\n",
       " 'a.a',\n",
       " 'a.c',\n",
       " 'a.d',\n",
       " 'a.e',\n",
       " 'a.f',\n",
       " 'a.i',\n",
       " 'a.j',\n",
       " 'a.k.a',\n",
       " 'a.m',\n",
       " 'a.s',\n",
       " 'a.u',\n",
       " 'a0',\n",
       " 'a1',\n",
       " 'a1c',\n",
       " 'a2',\n",
       " 'a2s',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'a5',\n",
       " 'a6',\n",
       " 'a7',\n",
       " 'a8',\n",
       " 'a9',\n",
       " 'a_a',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaaaah',\n",
       " 'aaaah',\n",
       " 'aaaargh',\n",
       " 'aaagh',\n",
       " 'aaah',\n",
       " 'aaahh',\n",
       " 'aaahhh',\n",
       " 'aaargh',\n",
       " 'aac',\n",
       " 'aachen',\n",
       " 'aad',\n",
       " 'aada',\n",
       " 'aadland',\n",
       " 'aage',\n",
       " 'aagh',\n",
       " 'aah',\n",
       " 'aahh',\n",
       " 'aahhh',\n",
       " 'aahing',\n",
       " 'aahs',\n",
       " 'aak',\n",
       " 'aakash',\n",
       " 'aal',\n",
       " 'aalborg',\n",
       " 'aalen',\n",
       " 'aaliya',\n",
       " 'aaliyah',\n",
       " 'aalst',\n",
       " 'aam',\n",
       " 'aames',\n",
       " 'aamir',\n",
       " 'aan',\n",
       " 'aand',\n",
       " 'aang',\n",
       " 'aapa',\n",
       " 'aar',\n",
       " 'aarau',\n",
       " 'aarav',\n",
       " 'aardvark',\n",
       " 'aardvarks',\n",
       " 'aardwolf',\n",
       " 'aare',\n",
       " 'aargau',\n",
       " 'aargh',\n",
       " 'aarhus',\n",
       " 'aarohi',\n",
       " 'aaron',\n",
       " 'aaron_burr',\n",
       " 'aaron_copland',\n",
       " 'aaronic',\n",
       " 'aarons',\n",
       " 'aaronson',\n",
       " 'aarp',\n",
       " 'aarti',\n",
       " 'aas',\n",
       " 'aasim',\n",
       " 'aat',\n",
       " 'aatish',\n",
       " 'aau',\n",
       " 'aav',\n",
       " 'aav##',\n",
       " 'aave',\n",
       " 'aaya',\n",
       " 'aayan',\n",
       " 'ab',\n",
       " 'ab_aeterno',\n",
       " 'ab_fab',\n",
       " 'ab_initio',\n",
       " 'aba',\n",
       " 'abab',\n",
       " 'ababa',\n",
       " 'ababil',\n",
       " 'abac',\n",
       " 'abaca',\n",
       " 'abaci',\n",
       " 'aback',\n",
       " 'abaco',\n",
       " 'abacus',\n",
       " 'abacuses',\n",
       " 'abad',\n",
       " 'abada',\n",
       " 'abadan',\n",
       " 'abaddon',\n",
       " 'abadie',\n",
       " 'abaft',\n",
       " 'abagnale',\n",
       " 'abajo',\n",
       " 'abakan',\n",
       " 'abalone',\n",
       " 'abalones',\n",
       " 'aban',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandonments',\n",
       " 'abandons',\n",
       " 'abandonware',\n",
       " 'abar',\n",
       " 'abarth',\n",
       " 'abas',\n",
       " 'abase',\n",
       " 'abased',\n",
       " 'abasement',\n",
       " 'abashed',\n",
       " 'abashiri',\n",
       " 'abasi',\n",
       " 'abassi',\n",
       " 'abate',\n",
       " 'abated',\n",
       " 'abatement',\n",
       " 'abates',\n",
       " 'abating',\n",
       " 'abatis',\n",
       " 'abattoir',\n",
       " 'abattoirs',\n",
       " 'abaxial',\n",
       " 'abay',\n",
       " 'abaya',\n",
       " 'abaza',\n",
       " 'abb',\n",
       " 'abba',\n",
       " 'abbacy',\n",
       " 'abbas',\n",
       " 'abbasi',\n",
       " 'abbasid',\n",
       " 'abbatial',\n",
       " 'abbe',\n",
       " 'abbess',\n",
       " 'abbesses',\n",
       " 'abbeville',\n",
       " 'abbey',\n",
       " 'abbeys',\n",
       " 'abbi',\n",
       " 'abbie',\n",
       " 'abbies',\n",
       " 'abbot',\n",
       " 'abbots',\n",
       " 'abbotsford',\n",
       " 'abbott',\n",
       " 'abbottabad',\n",
       " 'abbotts',\n",
       " 'abboud',\n",
       " 'abbr',\n",
       " 'abbrev',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviates',\n",
       " 'abbreviating',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abbs',\n",
       " 'abbu',\n",
       " 'abbud',\n",
       " 'abby',\n",
       " 'abbé',\n",
       " 'abc',\n",
       " 'abcs',\n",
       " 'abd',\n",
       " 'abdal',\n",
       " 'abdalla',\n",
       " 'abdallah',\n",
       " 'abdel',\n",
       " 'abdelkader',\n",
       " 'abdera',\n",
       " 'abdi',\n",
       " 'abdias',\n",
       " 'abdicate',\n",
       " 'abdicated',\n",
       " 'abdicates',\n",
       " 'abdicating',\n",
       " 'abdication',\n",
       " 'abdo',\n",
       " 'abdomen',\n",
       " 'abdomens',\n",
       " 'abdominal',\n",
       " 'abdominal_aorta',\n",
       " 'abdominal_aortic_aneurysm',\n",
       " 'abdominal_cavity',\n",
       " 'abdominal_muscle',\n",
       " 'abdominal_muscles',\n",
       " 'abdominal_wall',\n",
       " 'abdominals',\n",
       " 'abdominoplasty',\n",
       " 'abdon',\n",
       " 'abducens',\n",
       " 'abduct',\n",
       " 'abducted',\n",
       " 'abductee',\n",
       " 'abductees',\n",
       " 'abducting',\n",
       " 'abduction',\n",
       " 'abductions',\n",
       " 'abductive',\n",
       " 'abductor',\n",
       " 'abductors',\n",
       " 'abducts',\n",
       " 'abdul',\n",
       " 'abdulaziz',\n",
       " 'abdulkarim',\n",
       " 'abdullah',\n",
       " 'abdullatif',\n",
       " 'abe',\n",
       " 'abeam',\n",
       " 'abec',\n",
       " 'abecedarian',\n",
       " 'abed',\n",
       " 'abednego',\n",
       " 'abel',\n",
       " 'abel_tasman',\n",
       " 'abelard',\n",
       " 'abelardo',\n",
       " 'abele',\n",
       " 'abelian',\n",
       " 'abenaki',\n",
       " 'abend',\n",
       " 'abenomics',\n",
       " 'abeokuta',\n",
       " 'aber',\n",
       " 'abercrombie',\n",
       " 'aberdare',\n",
       " 'aberdeen',\n",
       " 'aberdeen_angus',\n",
       " 'aberdeenshire',\n",
       " 'aberdonian',\n",
       " 'aberg',\n",
       " 'abergavenny',\n",
       " 'abernathy',\n",
       " 'abernethy',\n",
       " 'aberrant',\n",
       " 'aberration',\n",
       " 'aberrational',\n",
       " 'aberrations',\n",
       " 'aberystwyth',\n",
       " 'abet',\n",
       " 'abetment',\n",
       " 'abetted',\n",
       " 'abetter',\n",
       " 'abetting',\n",
       " 'abettor',\n",
       " 'abeyance',\n",
       " 'abg',\n",
       " 'abgar',\n",
       " 'abh',\n",
       " 'abhay',\n",
       " 'abhi',\n",
       " 'abhimanyu',\n",
       " 'abhinaya',\n",
       " 'abhishek',\n",
       " 'abhor',\n",
       " 'abhorred',\n",
       " 'abhorrence',\n",
       " 'abhorrent',\n",
       " 'abhorring',\n",
       " 'abhors',\n",
       " 'abi',\n",
       " 'abib',\n",
       " 'abid',\n",
       " 'abide',\n",
       " 'abided',\n",
       " 'abides',\n",
       " 'abidin',\n",
       " 'abiding',\n",
       " 'abidjan',\n",
       " 'abie',\n",
       " 'abies',\n",
       " 'abig',\n",
       " 'abigail',\n",
       " 'abilene',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abimelech',\n",
       " 'abin',\n",
       " 'abingdon',\n",
       " 'abington',\n",
       " 'abiogenesis',\n",
       " 'abiotic',\n",
       " 'abir',\n",
       " 'abit',\n",
       " 'abitibi',\n",
       " 'abitur',\n",
       " 'abject',\n",
       " 'abjection',\n",
       " 'abjectly',\n",
       " 'abjuration',\n",
       " 'abjure',\n",
       " 'abjured',\n",
       " 'abkhaz',\n",
       " 'abkhazia',\n",
       " 'abkhazian',\n",
       " 'abkhazians',\n",
       " 'abla',\n",
       " 'ablate',\n",
       " 'ablated',\n",
       " 'ablation',\n",
       " 'ablative',\n",
       " 'ablaut',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'able_bodied',\n",
       " 'able_seaman',\n",
       " 'abled',\n",
       " 'ableism',\n",
       " 'ableist',\n",
       " 'ableman',\n",
       " 'abler',\n",
       " 'ablest',\n",
       " 'ablow',\n",
       " 'ablution',\n",
       " 'ablutions',\n",
       " 'ably',\n",
       " 'abm',\n",
       " 'abnegation',\n",
       " 'abner',\n",
       " 'abnormal',\n",
       " 'abnormal_psychology',\n",
       " 'abnormalities',\n",
       " 'abnormality',\n",
       " 'abnormally',\n",
       " 'abnormals',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abodes',\n",
       " 'abogado',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishing',\n",
       " 'abolishment',\n",
       " 'abolition',\n",
       " 'abolitionism',\n",
       " 'abolitionist',\n",
       " 'abolitionists',\n",
       " 'abomasum',\n",
       " 'abomey',\n",
       " 'abominable',\n",
       " 'abominable_snowman',\n",
       " 'abominably',\n",
       " 'abomination',\n",
       " 'abominations',\n",
       " 'abondance',\n",
       " 'aboot',\n",
       " 'abor',\n",
       " 'aboral',\n",
       " 'abord',\n",
       " 'aboriginal',\n",
       " 'aboriginality',\n",
       " 'aboriginals',\n",
       " 'aborigine',\n",
       " 'aborigines',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'abortifacient',\n",
       " 'aborting',\n",
       " 'abortion',\n",
       " 'abortion_clinic',\n",
       " 'abortion_pill',\n",
       " 'abortionist',\n",
       " 'abortions',\n",
       " 'abortive',\n",
       " 'aborts',\n",
       " 'abou',\n",
       " 'aboukir',\n",
       " 'abound',\n",
       " 'abounded',\n",
       " 'abounding',\n",
       " 'abounds',\n",
       " 'about',\n",
       " 'about_time',\n",
       " 'abouta',\n",
       " 'aboutness',\n",
       " 'abouts',\n",
       " 'aboutthe',\n",
       " 'above',\n",
       " 'above_average',\n",
       " 'above_reproach',\n",
       " 'aboveboard',\n",
       " 'aboveground',\n",
       " 'abovementioned',\n",
       " 'abq',\n",
       " 'abr',\n",
       " 'abra',\n",
       " 'abracadabra',\n",
       " 'abrade',\n",
       " 'abraded',\n",
       " 'abrading',\n",
       " 'abraham',\n",
       " 'abraham_lincoln',\n",
       " 'abrahamian',\n",
       " 'abrahamic',\n",
       " 'abrahamic_religions',\n",
       " 'abrahams',\n",
       " 'abrahamsson',\n",
       " 'abram',\n",
       " 'abramoff',\n",
       " 'abramov',\n",
       " 'abramovich',\n",
       " 'abramowitz',\n",
       " 'abrams',\n",
       " 'abramsky',\n",
       " 'abramson',\n",
       " 'abrash',\n",
       " 'abrasion',\n",
       " 'abrasions',\n",
       " 'abrasive',\n",
       " 'abrasively',\n",
       " 'abrasiveness',\n",
       " 'abrasives',\n",
       " 'abraxas',\n",
       " 'abrazo',\n",
       " 'abreaction',\n",
       " 'abreast',\n",
       " 'abrego',\n",
       " 'abreu',\n",
       " 'abri',\n",
       " 'abridge',\n",
       " 'abridged',\n",
       " 'abridgement',\n",
       " 'abridges',\n",
       " 'abridging',\n",
       " 'abridgment',\n",
       " 'abrin',\n",
       " 'abroad',\n",
       " 'abrogate',\n",
       " 'abrogated',\n",
       " 'abrogating',\n",
       " 'abrogation',\n",
       " 'abrupt',\n",
       " 'abruption',\n",
       " 'abruptly',\n",
       " 'abruptness',\n",
       " 'abruzzi',\n",
       " 'abruzzo',\n",
       " 'abs',\n",
       " 'absalom',\n",
       " 'absalon',\n",
       " 'absaroka',\n",
       " 'abscam',\n",
       " 'abscess',\n",
       " 'abscessed',\n",
       " 'abscessed_tooth',\n",
       " 'abscesses',\n",
       " 'abscisic',\n",
       " 'abscisic_acid',\n",
       " 'abscissa',\n",
       " 'abscission',\n",
       " 'abscond',\n",
       " 'absconded',\n",
       " 'absconder',\n",
       " 'absconding',\n",
       " 'absconds',\n",
       " 'abseil',\n",
       " 'abseiling',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absent_minded',\n",
       " 'absent_mindedly',\n",
       " 'absent_mindedness',\n",
       " 'absentee',\n",
       " 'absentee_ballot',\n",
       " 'absentee_voter',\n",
       " 'absentee_voting',\n",
       " 'absenteeism',\n",
       " 'absentees',\n",
       " 'absentia',\n",
       " 'absenting',\n",
       " 'absently',\n",
       " 'absentminded',\n",
       " 'absentmindedly',\n",
       " 'absentmindedness',\n",
       " 'absinth',\n",
       " 'absinthe',\n",
       " 'abso',\n",
       " 'absolon',\n",
       " 'absolut',\n",
       " 'absolut_vodka',\n",
       " 'absolute',\n",
       " 'absolute_monarchy',\n",
       " 'absolute_zero',\n",
       " 'absolutely',\n",
       " 'absoluteness',\n",
       " 'absolutes',\n",
       " 'absolution',\n",
       " 'absolutism',\n",
       " 'absolutist',\n",
       " 'absolutive',\n",
       " 'absolutly',\n",
       " 'absolve',\n",
       " 'absolved',\n",
       " 'absolves',\n",
       " 'absolving',\n",
       " 'absorb',\n",
       " 'absorbable',\n",
       " 'absorbance',\n",
       " 'absorbant',\n",
       " 'absorbed',\n",
       " 'absorbency',\n",
       " 'absorbent',\n",
       " 'absorbent_material',\n",
       " 'absorber',\n",
       " 'absorbers',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorptiometry',\n",
       " 'absorption',\n",
       " 'absorptive',\n",
       " 'absorptivity',\n",
       " 'abstain',\n",
       " 'abstained',\n",
       " 'abstainer',\n",
       " 'abstainers',\n",
       " 'abstaining',\n",
       " 'abstains',\n",
       " 'abstemious',\n",
       " 'abstention',\n",
       " 'abstentions',\n",
       " 'abstinence',\n",
       " 'abstinence_only',\n",
       " 'abstinent',\n",
       " 'abstract',\n",
       " 'abstract_expressionism',\n",
       " 'abstract_expressionist',\n",
       " 'abstract_number',\n",
       " 'abstracted',\n",
       " 'abstracting',\n",
       " 'abstraction',\n",
       " 'abstractionism',\n",
       " 'abstractionist',\n",
       " 'abstractions',\n",
       " 'abstractly',\n",
       " 'abstractness',\n",
       " 'abstracts',\n",
       " 'abstruse',\n",
       " 'absurd',\n",
       " 'absurdism',\n",
       " 'absurdist',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdum',\n",
       " 'abt',\n",
       " 'abu',\n",
       " 'abu_bakr',\n",
       " 'abu_dhabi',\n",
       " 'abu_qasim',\n",
       " 'abu_sayyaf',\n",
       " 'abu_simbel',\n",
       " 'abubakar',\n",
       " 'abuela',\n",
       " 'abuelita',\n",
       " 'abuelo',\n",
       " 'abuja',\n",
       " 'abul',\n",
       " 'abun',\n",
       " 'abuna',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abusiveness',\n",
       " 'abut',\n",
       " 'abutilon',\n",
       " 'abutment',\n",
       " 'abuts',\n",
       " 'abutted',\n",
       " 'abutting',\n",
       " 'abuzz',\n",
       " 'abv',\n",
       " 'abwehr',\n",
       " 'aby',\n",
       " 'abydos',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'abyssal',\n",
       " 'abyssinia',\n",
       " 'abyssinian',\n",
       " 'ac',\n",
       " 'ac##',\n",
       " 'ac_dc',\n",
       " 'aca',\n",
       " 'acacia',\n",
       " 'acacias',\n",
       " 'acad',\n",
       " 'academe',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academicals',\n",
       " 'academician',\n",
       " 'academicism',\n",
       " 'academics',\n",
       " 'academie',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'academy_award',\n",
       " 'academy_awards',\n",
       " 'acadia',\n",
       " 'acadia_national_park',\n",
       " 'acadian',\n",
       " 'acadians',\n",
       " 'académie_française',\n",
       " 'acai',\n",
       " 'acai_berry',\n",
       " 'acamprosate',\n",
       " 'acanthaceae',\n",
       " 'acanthamoeba',\n",
       " 'acanthocephala',\n",
       " 'acanthosis',\n",
       " 'acanthus',\n",
       " 'acapulco',\n",
       " 'acar',\n",
       " 'acara',\n",
       " 'acari',\n",
       " 'acarnania',\n",
       " 'acars',\n",
       " 'acas',\n",
       " 'acausal',\n",
       " 'acc',\n",
       " 'accede',\n",
       " 'accedes',\n",
       " 'acceding',\n",
       " 'accelerando',\n",
       " 'accelerant',\n",
       " 'accelerants',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerated_depreciation',\n",
       " 'accelerates',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerations',\n",
       " 'accelerator',\n",
       " 'accelerator_mass_spectrometry',\n",
       " 'accelerator_pedal',\n",
       " 'accelerators',\n",
       " 'accelerometer',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accenting',\n",
       " 'accents',\n",
       " 'accentual',\n",
       " 'accentuate',\n",
       " 'accentuated',\n",
       " 'accentuates',\n",
       " 'accentuating',\n",
       " 'accentuation',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptably',\n",
       " 'acceptance',\n",
       " 'acceptances',\n",
       " 'acceptation',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'acception',\n",
       " 'acceptor',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'access_control',\n",
       " 'accessed',\n",
       " 'accesses',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessibly',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessions',\n",
       " 'accessor',\n",
       " 'accessories',\n",
       " 'accessorise',\n",
       " 'accessorize',\n",
       " 'accessorized',\n",
       " 'accessorizing',\n",
       " 'accessory',\n",
       " 'acci',\n",
       " 'accident',\n",
       " 'accident_blackspot',\n",
       " 'accidental',\n",
       " 'accidental_death',\n",
       " 'accidentally',\n",
       " 'accidentals',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'accio',\n",
       " 'accipiter',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclaiming',\n",
       " 'acclaims',\n",
       " 'acclamation',\n",
       " 'acclamations',\n",
       " 'acclimatation',\n",
       " 'acclimate',\n",
       " 'acclimated',\n",
       " 'acclimating',\n",
       " 'acclimation',\n",
       " 'acclimatisation',\n",
       " 'acclimatise',\n",
       " 'acclimatization',\n",
       " 'acclimatize',\n",
       " 'acclimatized',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'accommodative',\n",
       " 'accomodate',\n",
       " 'accomodating',\n",
       " 'accomodation',\n",
       " 'accomodations',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompanist',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accompli',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accordant',\n",
       " 'accorded',\n",
       " 'accordian',\n",
       " 'accordin',\n",
       " 'according',\n",
       " 'according_to',\n",
       " 'accordingly',\n",
       " 'accordingto',\n",
       " 'accordion',\n",
       " 'accordionist',\n",
       " 'accordions',\n",
       " 'accords',\n",
       " 'accost',\n",
       " 'accosted',\n",
       " 'accosting',\n",
       " 'accosts',\n",
       " 'account',\n",
       " 'account_executive',\n",
       " 'account_for',\n",
       " 'account_manager',\n",
       " 'account_number',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountancy',\n",
       " 'accountant',\n",
       " 'accountant_general',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounting_firm',\n",
       " 'accounting_principle',\n",
       " 'accounts',\n",
       " 'accounts_payable',\n",
       " 'accounts_receivable',\n",
       " 'accoutrement',\n",
       " 'accoutrements',\n",
       " 'accra',\n",
       " 'accredit',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accrediting',\n",
       " 'accrete',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5a0df-713b-44dc-95e3-b1da60d0b795",
   "metadata": {},
   "source": [
    "Now, we are going to normalize the vectors. Here, the focus is not on the length of them (which represents frequency of use), but in the direction and proximity of them. So, normalizing them and using the dot product as a form of calculating the similarity, as it is proportional to the cossine of the angle between the vectors, is the strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4a289c-f6f5-41ca-bcc1-fc71b8be8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a1c10d-0475-44e3-a020-5896b1a39b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dic that maps each word to its index\n",
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd2c489c-50ab-40df-8557-256d59a2ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the similarity between words:\n",
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b10dce5-865b-4a21-b79e-a850630f15bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67598516"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"blue\", \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f76504da-7da0-4b28-a912-d41f8e3f995b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81995475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"cat\", \"feline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a656481-53d0-41db-9755-e0e347d31e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013479051"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"soccer\", \"steak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd5cdc11-3ed2-4ee8-b3c8-500f261ba689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"cat\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09188cfe-c8c3-4a6d-a925-10d6cb93cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words to a given word\n",
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
    "    return best_words[:n]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad5b3810-3eab-4828-8155-85815b6a7286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'humane_society',\n",
       " 'kitten',\n",
       " 'feline',\n",
       " 'colocolo',\n",
       " 'cats',\n",
       " 'kitty',\n",
       " 'maine_coon',\n",
       " 'housecat',\n",
       " 'sharp_teeth']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cat\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c69519f1-a05e-4fbd-9bbc-cfb540362ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soccer',\n",
       " 'fa_cup',\n",
       " 'football',\n",
       " 'soccerball',\n",
       " 'white_hart_lane',\n",
       " 'footballs',\n",
       " 'toe_poke',\n",
       " 'footgolf',\n",
       " 'bafana_bafana',\n",
       " 'footballing']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"soccer\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d36dc655-712d-472e-a552-9e80b028e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'unconditional_love', 'loves', 'unlove', 'loved']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"love\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f35dabe-9a07-40a7-a74c-171fb1bfed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding words that are close to tuple of words\n",
    "def closest_to_vector_ndim(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closest_to_vector(b2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17fe8e14-2731-40c7-af51-d36ed7d7a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sister']\n"
     ]
    }
   ],
   "source": [
    "print(closest_to_vector_ndim(\"man\", \"brother\", \"woman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "079adbb3-dfd3-46a7-b5df-51630cee76b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_to_vector_ndim(\"spain\", \"madrid\", \"france\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7039-77c5-4445-b94a-14f4bcf4d5f8",
   "metadata": {},
   "source": [
    "Now, we are going to perform sentiment analysis based on a txt that contains movie reviews and classifications (0 for negative and 1 for positive). Here, SWEM (Simple Word Embedding Model) is used, that is, we are going to take the mean of each word vector in the document and used it in a logistic regression. Let's read the txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39ba83f7-c395-426a-ba74-c3ded9705e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    \n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    \n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return x, y\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "xs = []\n",
    "ys = []\n",
    "with open(\"movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    for l in f.readlines():\n",
    "        x, y = convert_line_to_example(l)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "# Concatenate all examples into a numpy array\n",
    "xs = np.vstack(xs)\n",
    "ys = np.vstack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "481d7919-bf31-4a18-9ad4-c742a137c472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1411, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb9faf1f-e542-4819-9b8f-2c411438ffe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1411, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d974959-de76-48bc-beb3-c0969839eaad",
   "metadata": {},
   "source": [
    "So, we have 1411 reviews..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bf995-ea28-4df8-88de-a0e5a18cabc7",
   "metadata": {},
   "source": [
    "Now, let's make train-test split. First, we have to shuffle the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9db123d2-6654-4c02-a384-125129d1c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(xs.shape[0])\n",
    "xs = xs[shuffle_idx, :]\n",
    "ys = ys[shuffle_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d699f9-e7fe-4c02-8e82-373ca83deedf",
   "metadata": {},
   "source": [
    "Converting the data to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f4bbe07-bee3-4baf-848f-f3b354070048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_train = 4*xs.shape[0] // 5\n",
    "\n",
    "x_train = torch.tensor(xs[:num_train])\n",
    "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(xs[num_train:])\n",
    "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0260725f-6e69-42ea-91be-ccb6106a38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31752bac-3ac9-42f7-a294-de81d78012cb",
   "metadata": {},
   "source": [
    "Let's build SWEM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fe8a861-3f52-4379-a7a3-5b6bb52673eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41c8078e-7376-43b5-a924-aae49ba42e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 1) # One prediction on the final. Passing on a sigmoid function, 0.5 will be the threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdb52b-3e25-48b6-bb8b-8bd790de401c",
   "metadata": {},
   "source": [
    "Training the model and printing the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "278c5e44-fe45-41b4-9910-03c3dcac016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss: 0.6902092099189758 \t Train Acc: 0.5647163391113281\n",
      "Epoch: 25 \t Train Loss: 0.273639440536499 \t Train Acc: 0.9468085169792175\n",
      "Epoch: 50 \t Train Loss: 0.14503586292266846 \t Train Acc: 0.9689716100692749\n",
      "Epoch: 75 \t Train Loss: 0.11983732134103775 \t Train Acc: 0.9778369069099426\n",
      "Epoch: 100 \t Train Loss: 0.030818801373243332 \t Train Acc: 0.9804964661598206\n",
      "Epoch: 125 \t Train Loss: 0.033565063029527664 \t Train Acc: 0.9831560254096985\n",
      "Epoch: 150 \t Train Loss: 0.06276717036962509 \t Train Acc: 0.9858155846595764\n",
      "Epoch: 175 \t Train Loss: 0.014347316697239876 \t Train Acc: 0.9902482032775879\n",
      "Epoch: 200 \t Train Loss: 0.05792086198925972 \t Train Acc: 0.9929078221321106\n",
      "Epoch: 225 \t Train Loss: 0.06382538378238678 \t Train Acc: 0.9946808218955994\n",
      "Test accuracy: 0.9575971961021423\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "# Instantiate model\n",
    "model = SWEM()\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and Adam Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(250):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_examples += len(inputs)\n",
    "    \n",
    "    # Print training progress\n",
    "    if epoch % 25 == 0:\n",
    "        acc = correct/num_examples\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(inputs)\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da010ee0-5c26-4404-90ea-99aa98f402ee",
   "metadata": {},
   "source": [
    "Checking what our model has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1d30bef-af2a-4a7f-abf4-138cf0bf299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of the word 'exciting': tensor([[1.]], grad_fn=<SigmoidBackward0>)\n",
      "Sentiment of the word 'hated': tensor([[8.6474e-20]], grad_fn=<SigmoidBackward0>)\n",
      "Sentiment of the word 'boring': tensor([[4.6554e-14]], grad_fn=<SigmoidBackward0>)\n",
      "Sentiment of the word 'loved': tensor([[1.]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))\n",
    "    print(\"Sentiment of the word '{0}': {1}\".format(word, torch.sigmoid(model(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e84f6-851d-418d-8a92-d12e0803f54f",
   "metadata": {},
   "source": [
    "# Learning word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92ca37-4bf3-43e0-a32f-b751b7ad5290",
   "metadata": {},
   "source": [
    "Now, we are going to make the word vectors parameters of our model, and we are going to learn than in the training phase. Training the word vectors makes them more specific to the task in focus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee744044-2e0d-481e-8985-04228adccefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 300])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBED_DIM = 300\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "embedding.weight.size() # Matrix with 5000 word vectors with 300 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c9741-fe74-4bad-aff7-74ed1e59109b",
   "metadata": {},
   "source": [
    "SWEM model in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56b47b24-bc9b-44a8-b84c-5bb443e8e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEMWithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80081b-0b09-47df-84a2-71a6c17ce8db",
   "metadata": {},
   "source": [
    "Specifying the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94648f75-64b9-4c21-ab32-5d5efb8c464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEMWithEmbeddings(\n",
      "  (embedding): Embedding(5000, 300)\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SWEMWithEmbeddings(\n",
    "    vocab_size = 5000,\n",
    "    embedding_size = 300, \n",
    "    hidden_dim = 64, \n",
    "    num_outputs = 1,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210462c3-5c96-4b4c-9758-03b7b848d3e8",
   "metadata": {},
   "source": [
    "## RNN's (Recurrent Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf96f4c-624f-47f2-8a29-e5f8f4023a0f",
   "metadata": {},
   "source": [
    "RNN's are used in the case of predicting and infering sequential data, which is the case, in most of time, of NLP. So, let's do it with the phrase: \"Recurrent neural networks are great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24909b8c-8c38-4ec6-a244-ad148a1fdeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs shape: torch.Size([5, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "mb = 1\n",
    "x_dim = 300 \n",
    "sentence = [\"recurrent\", \"neural\", \"networks\", \"are\", \"great\"]\n",
    "\n",
    "xs = []\n",
    "for word in sentence:\n",
    "    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))\n",
    "    \n",
    "xs = torch.stack(xs, dim=0)\n",
    "print(\"xs shape: {}\".format(xs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "087077a5-9251-4e45-86f8-94516f4f181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0527,  ...,  0.0176,  0.0351,  0.0176]],\n",
       "\n",
       "        [[ 0.0175,  0.0000,  0.0350,  ..., -0.0525,  0.0175,  0.0350]],\n",
       "\n",
       "        [[ 0.0174,  0.0000,  0.0000,  ...,  0.0174,  0.0174, -0.0348]],\n",
       "\n",
       "        [[ 0.0000, -0.0347,  0.0521,  ..., -0.0868,  0.0174, -0.0347]],\n",
       "\n",
       "        [[ 0.0000, -0.0172,  0.0345,  ...,  0.0000, -0.1552, -0.1207]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259865d-6865-4af4-9208-a50bd4c58162",
   "metadata": {},
   "source": [
    "One vector for each word in the phrase with 300 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71d680-a928-4a3e-8c2b-e0fb45dc26e0",
   "metadata": {},
   "source": [
    "Building a RNN in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3314af5f-dc7a-42a7-9fe3-049e0b10b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "h_dim = 128\n",
    "\n",
    "# For projecting the input\n",
    "Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)\n",
    "Wx.requires_grad_()\n",
    "bx = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "# For projecting the previous state\n",
    "Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)\n",
    "Wh.requires_grad_()\n",
    "bh = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "h_dim = 128\n",
    "\n",
    "# For projecting the input\n",
    "Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)\n",
    "Wx.requires_grad_()\n",
    "bx = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "# For projecting the previous state\n",
    "Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)\n",
    "Wh.requires_grad_()\n",
    "bh = torch.zeros(h_dim, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b33b0e-763f-4e6b-90f4-ff31f8fbfd9b",
   "metadata": {},
   "source": [
    "Let's make a function that will do the time step in the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2da2944-45d1-46a0-9e48-a09232b7545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_step(x, h):\n",
    "    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))\n",
    "\n",
    "    return h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a78e04-244f-42af-a053-752df669411d",
   "metadata": {},
   "source": [
    "First step, that is, beggining of sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "31680429-e7b7-47c4-8c10-755a3a21a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding for first word\n",
    "x1 = xs[0, :, :]\n",
    "\n",
    "# Initialize hidden state to 0\n",
    "h0 = torch.zeros([mb, h_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81566ed7-5b44-4fc1-adee-0d3b6e0b720d",
   "metadata": {},
   "source": [
    "Forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "009ab6c0-b494-4d4f-831a-5772878f170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state h1 dimensions: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass of one RNN step for time step t=1\n",
    "h1 = RNN_step(x1, h0)\n",
    "\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6296bf-5307-4ee4-a01e-bd00bfb7c069",
   "metadata": {},
   "source": [
    "Forward again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "baedfd49-ed53-4603-978a-1117803e124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state h2 dimensions: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Word embedding for second word\n",
    "x2 = xs[1, :, :]\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "h2 = RNN_step(x2, h1)\n",
    "\n",
    "print(\"Hidden state h2 dimensions: {0}\".format(h2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae27b3-0f43-42d2-86d9-3ffd55f47aa9",
   "metadata": {},
   "source": [
    "So, we can continue, feeding the word vector input of the moment, as well as the hidden unit from the last phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24de64-89a6-492b-8084-57d7a533efa0",
   "metadata": {},
   "source": [
    "We can use high level API's presented in PyTorch, that already have RNN's implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6534b986-7b22-41eb-9553-daf710435af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN parameter shapes: [torch.Size([128, 300]), torch.Size([128, 128]), torch.Size([128]), torch.Size([128])]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "\n",
    "rnn = nn.RNN(x_dim, h_dim)\n",
    "print(\"RNN parameter shapes: {}\".format([p.shape for p in rnn.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53dc6b-283b-42b5-bf4b-8e88f0709f23",
   "metadata": {},
   "source": [
    "Forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b4ed49a-60bc-4e57-a636-06e9e3f7bf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states shape: torch.Size([5, 1, 128])\n",
      "Final hidden state shape: torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "hs, h_T = rnn(xs)\n",
    "\n",
    "print(\"Hidden states shape: {}\".format(hs.shape))\n",
    "print(\"Final hidden state shape: {}\".format(h_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e78d39-eb07-4ad1-b0c7-bd9a023d7ba9",
   "metadata": {},
   "source": [
    "Example of LSTM using the torch.nn API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85a58c87-9d2c-48b3-8ad5-cb504e0eb1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM parameters: [torch.Size([512, 300]), torch.Size([512, 128]), torch.Size([512]), torch.Size([512])]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(x_dim, h_dim)\n",
    "print(\"LSTM parameters: {}\".format([p.shape for p in lstm.parameters()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
